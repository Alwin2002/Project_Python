{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usgWqi42bJAy",
        "outputId": "7d513067-735f-4c11-9022-3bceacba4c24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "def unet(pretrained_weights = None,input_size = (512,512,1)):\n",
        "    inputs = Input(input_size)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "    merge6 = concatenate([drop4,up6], axis = 3)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "\n",
        "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs,conv10)\n",
        "\n",
        "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "\n",
        "    if(pretrained_weights):\n",
        "      model.load_weights(pretrained_weights)\n",
        "\n",
        "    return model\n",
        "\n",
        "model1=unet()\n",
        "model1.load_weights('/content/drive/MyDrive/trainedmodes/text_seg_model.h5')\n",
        "\n",
        "\n",
        "line_img_array=[]\n",
        "\n",
        "\n",
        "def segment_into_lines(filename):\n",
        "    \n",
        "    img=cv2.imread(f'{filename}',0)\n",
        "    ret,img=cv2.threshold(img,150,255,cv2.THRESH_BINARY_INV)\n",
        "    img=cv2.resize(img,(512,512))\n",
        "   \n",
        "    img= np.expand_dims(img,axis=-1)\n",
        "\n",
        "    img=np.expand_dims(img,axis=0)\n",
        "    pred=model1.predict(img)\n",
        "    pred=np.squeeze(np.squeeze(pred,axis=0),axis=-1)\n",
        "\n",
        "    \n",
        "\n",
        "    coordinates=[]\n",
        "    img = cv2.normalize(src=pred, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
        "    cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU,img)\n",
        "    ori_img=cv2.imread(f'{filename}',0)\n",
        " \n",
        "\n",
        "    (H, W) = ori_img.shape[:2]\n",
        "    (newW, newH) = (512, 512)\n",
        "    rW = W / float(newW)\n",
        "    rH = H / float(newH)\n",
        "    \n",
        "    contours, hier = cv2.findContours(img, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
        "    for c in contours:\n",
        "        # get the bounding rect\n",
        "        x, y, w, h = cv2.boundingRect(c)\n",
        "        #cv2.rectangle(ori_img, (int(x*rW), int(y*rH)), (int((x+w)*rW),int((y+h)*rH)), (255,0,0), 1)\n",
        "        coordinates.append((int(x*rW),int(y*rH),int((x+w)*rW),int((y+h)*rH)))\n",
        "    #cv2.imwrite(\"output.jpg\",ori_img)\n",
        "\n",
        "    for i in range(len(coordinates)-1,-1,-1):\n",
        "        coors=coordinates[i]\n",
        "\n",
        "        p_img=ori_img[coors[1]:coors[3],coors[0]:coors[2]].copy()\n",
        "\n",
        "        line_img_array.append(p_img)\n",
        "\n",
        "    return line_img_array"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from spellchecker import SpellChecker\n",
        "import math\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def find_dominant_color(image):\n",
        "        #Resizing parameters\n",
        "        width, height = 150,150\n",
        "        image = image.resize((width, height),resample = 0)\n",
        "        #Get colors from image object\n",
        "        pixels = image.getcolors(width * height)\n",
        "        #Sort them by count number(first element of tuple)\n",
        "        sorted_pixels = sorted(pixels, key=lambda t: t[0])\n",
        "        #Get the most frequent color\n",
        "        dominant_color = sorted_pixels[-1][1]\n",
        "        return dominant_color\n",
        "\n",
        "def preprocess_img(img, imgSize):\n",
        "    \"put img into target img of size imgSize, transpose for TF and normalize gray-values\"\n",
        "\n",
        "    # there are damaged files in IAM dataset - just use black image instead\n",
        "    if img is None:\n",
        "        img = np.zeros([imgSize[1], imgSize[0]]) \n",
        "        print(\"Image None!\")\n",
        "\n",
        "    # create target image and copy sample image into it\n",
        "    (wt, ht) = imgSize\n",
        "    (h, w) = img.shape\n",
        "    fx = w / wt\n",
        "    fy = h / ht\n",
        "    f = max(fx, fy)\n",
        "    newSize = (max(min(wt, int(w / f)), 1),\n",
        "               max(min(ht, int(h / f)), 1))  # scale according to f (result at least 1 and at most wt or ht)\n",
        "    img = cv2.resize(img, newSize, interpolation=cv2.INTER_CUBIC) # INTER_CUBIC interpolation best approximate the pixels image\n",
        "                                                               # see this https://stackoverflow.com/a/57503843/7338066\n",
        "    most_freq_pixel=find_dominant_color(Image.fromarray(img))\n",
        "    target = np.ones([ht, wt]) * most_freq_pixel  \n",
        "    target[0:newSize[1], 0:newSize[0]] = img\n",
        "\n",
        "    img = target\n",
        "\n",
        "    return img\n",
        "\n",
        "def pad_img(img):\n",
        "    old_h,old_w=img.shape[0],img.shape[1]\n",
        "\n",
        "    #Pad the height.\n",
        "\n",
        "    #If height is less than 512 then pad to 512\n",
        "    if old_h<512:\n",
        "        to_pad=np.ones((512-old_h,old_w))*255\n",
        "        img=np.concatenate((img,to_pad))\n",
        "        new_height=512\n",
        "    else:\n",
        "    #If height >512 then pad to nearest 10.\n",
        "        to_pad=np.ones((roundup(old_h)-old_h,old_w))*255\n",
        "        img=np.concatenate((img,to_pad))\n",
        "        new_height=roundup(old_h)\n",
        "\n",
        "    #Pad the width.\n",
        "    if old_w<512:\n",
        "        to_pad=np.ones((new_height,512-old_w))*255\n",
        "        img=np.concatenate((img,to_pad),axis=1)\n",
        "        new_width=512\n",
        "    else:\n",
        "        to_pad=np.ones((new_height,roundup(old_w)-old_w))*255\n",
        "        img=np.concatenate((img,to_pad),axis=1)\n",
        "        new_width=roundup(old_w)-old_w\n",
        "    return img\n",
        "\n",
        "def roundup(x):\n",
        "    return int(math.ceil(x / 10.0)) * 10\n",
        "\n",
        "def unet(pretrained_weights = None,input_size = (512,512,1)):\n",
        "    inputs = Input(input_size)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "    merge6 = concatenate([drop4,up6], axis = 3)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "\n",
        "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs,conv10)\n",
        "\n",
        "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "\n",
        "    if(pretrained_weights):\n",
        "      model.load_weights(pretrained_weights)\n",
        "\n",
        "    return model\n",
        "\n",
        "model=unet()\n",
        "model.load_weights('/content/drive/MyDrive/trainedmodes/word_seg_model.h5')\n",
        "\n",
        "\n",
        "\n",
        "def sort_word(wordlist):\n",
        "    wordlist.sort(key=lambda x:x[0])\n",
        "    return wordlist\n",
        "\n",
        "\n",
        "\n",
        "def segment_into_words(line_img,idx):\n",
        "    \"\"\"This function takes in the line image and line index returns word images and the reference\n",
        "    of line they belong to.\"\"\"\n",
        "    img=pad_img(line_img)\n",
        "    ori_img=img.copy()\n",
        "    #ori_img=np.stack((ori_img,)*3, axis=-1)\n",
        "    ret,img=cv2.threshold(img,150,255,cv2.THRESH_BINARY_INV)\n",
        "    \n",
        "    img=cv2.resize(img,(512,512))\n",
        "    img=np.expand_dims(img,axis=-1)\n",
        "    img=img/255\n",
        "    img=np.expand_dims(img,axis=0)\n",
        "    seg_pred=model.predict(img)\n",
        "    seg_pred=np.squeeze(np.squeeze(seg_pred,axis=0),axis=-1)\n",
        "    seg_pred=cv2.normalize(src=seg_pred, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
        "    cv2.threshold(seg_pred,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU,seg_pred)\n",
        "    contours, hier = cv2.findContours(seg_pred, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
        "    \n",
        "    (H, W) = ori_img.shape[:2]\n",
        "    (newW, newH) = (512, 512)\n",
        "    rW = W / float(newW)\n",
        "    rH = H / float(newH)\n",
        "\n",
        "    coordinates=[]\n",
        "\n",
        "    for c in contours:\n",
        "        # get the bounding rect\n",
        "        x, y, w, h = cv2.boundingRect(c)\n",
        "        # draw a white rectangle to visualize the bounding rect\n",
        "        # cv2.rectangle(ori_img, (int(x*rW), int(y*rH)), (int((x+w)*rW),int((y+h)*rH)), (255,0,0), 1)\n",
        "        coordinates.append((int(x*rW),int(y*rH),int((x+w)*rW),int((y+h)*rH)))\n",
        "\n",
        "    coordinates=sort_word(coordinates)  #Sorting according to x-coordinates.\n",
        "    word_counter=0\n",
        "\n",
        "    word_array=[]\n",
        "    line_indicator=[]\n",
        "\n",
        "    for (x1,y1,x2,y2) in coordinates:\n",
        "        word_img=ori_img[y1:y2,x1:x2]\n",
        "        word_img=preprocess_img(word_img,(128,32))\n",
        "        word_img=np.expand_dims(word_img,axis=-1)\n",
        "        word_array.append(word_img)\n",
        "        line_indicator.append(idx)\n",
        "\n",
        "    return line_indicator,word_array"
      ],
      "metadata": {
        "id": "jWW002cwb20P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b7fc8cb-53ba-41ff-f2e3-39d8180d951a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 33.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import string\n",
        "import os\n",
        "\n",
        "\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional,Dropout\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "\n",
        "char_list = string.ascii_letters+string.digits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "inputs = Input(shape=(32,128,1))\n",
        " \n",
        "\n",
        "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "\n",
        "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        " \n",
        "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
        "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        " \n",
        "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
        " \n",
        "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "\n",
        "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        " \n",
        "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
        "\n",
        "batch_norm_5 = BatchNormalization()(conv_5)\n",
        " \n",
        "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "batch_norm_6 = BatchNormalization()(conv_6)\n",
        "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        " \n",
        "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        " \n",
        "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
        " \n",
        "\n",
        "blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
        "blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
        " \n",
        "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
        "\n",
        "\n",
        "act_model = Model(inputs, outputs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "act_model.load_weights('/content/drive/MyDrive/trainedmodes/CRNN_model.hdf5')\n",
        "\n",
        "\n",
        "\n",
        "def recognize_words(line_indicator,word_array,n_lines):\n",
        "\n",
        "    file=open('recognized_texts.txt','w')\n",
        "\n",
        "    line_rec=[]\n",
        "    for listidx in range(n_lines):\n",
        "        line_rec.append([])\n",
        "\n",
        "\n",
        "    predictions=act_model.predict(word_array)\n",
        "   \n",
        "\n",
        "    out = K.get_value(K.ctc_decode(predictions, input_length=np.ones(predictions.shape[0])*predictions.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        "\n",
        "    lw_idx=0\n",
        "   \n",
        "    for wordidxs in out:\n",
        "        word=[]\n",
        "        for char in wordidxs:\n",
        "            if int(char)!=-1:\n",
        "                word.append(char_list[int(char)])\n",
        "        word=spell.correction(''.join(word))\n",
        "   \n",
        "        line_rec[line_indicator[lw_idx]].append(word)\n",
        "        lw_idx+=1\n",
        "    \n",
        "    for listidx in range(n_lines):\n",
        "        line=line_rec[listidx]\n",
        "        print(line)"
      ],
      "metadata": {
        "id": "zii3laB6dV-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "line_img_array=segment_into_lines('temp.PNG')\n",
        "\n",
        "\n",
        "#Creating lists to store the line indexes,words list.\n",
        "full_index_indicator=[]\n",
        "all_words_list=[]\n",
        "#Variable to count the total no of lines in page.\n",
        "len_line_arr=0\n",
        "\n",
        "#Segment the lines into words and store as arrays.\n",
        "for idx,im in enumerate(line_img_array):\n",
        "    line_indicator,word_array=segment_into_words(im,idx)\n",
        "    for k in range(len(word_array)):\n",
        "        full_index_indicator.append(line_indicator[k])\n",
        "        all_words_list.append(word_array[k])\n",
        "    len_line_arr+=1\n",
        "    \n",
        "\n",
        "all_words_list=np.array(all_words_list)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geJPjtw2d67a",
        "outputId": "d1486419-0f9a-4919-b9fb-e69617dd7423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 5s 5s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recognize_words(full_index_indicator,all_words_list,len_line_arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVQ_wC1EiU1g",
        "outputId": "39801a1c-9f44-4bcc-ab33-0129ff4634b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 4s 510ms/step\n",
            "['there', 'are', 'man', 'variation', 'of', 'passage', 'of', 'tore', 'sum', 'available']\n",
            "['but', 'the', 'majority', 'have', 'suffered', 'alteration', 'in', 'some', 'form', 'be', 'infected']\n",
            "['humor', 'o', 'randomized', 'words', 'which', 'donlt', 'look', 'ever', 'slight']\n",
            "['i', 'i']\n",
            "['believable', 'i', 'vou', 'are', 'going', 'to', 'use', 'a', 'passage', 'of', 'tore', 'sum', 'vou']\n",
            "['need', 'to', 'be', 'sure', 'there', \"isn't\", 'anvthing', 'embarrassed', 'hidden', 'ip', 'the']\n",
            "['middle', 'of', 'text', 'al', 'the', 'tore', 'sum', 'gene', 'actors', 'or', 'the', 'lnternet']\n",
            "['i', 'i']\n",
            "['tend', 'to', 'repeat', None, 'chunk', 'as', 'necessary', 'make', 'this', 'the']\n",
            "['first', 'true', 'generator', 'on', 'the', 'lnternet', 'it', 'uses', 'a', 'dictionary', 'of', 'over', 'zoo']\n",
            "['tater', 'words', 'combined', 'ith', 'a', 'handful', 'i', 'mode', 'sentence']\n",
            "['i', 'i']\n",
            "['i', 'i']\n",
            "['i', 'i']\n",
            "['structures', 'to', 'generate', 'tore', 'sum', 'which', 'looks', 'reasonable', 'rhe']\n",
            "['generate', 'tore', 'sum', 'is', 'therefore', 'alive', 'free', 'from', 'repetition']\n",
            "['infected', 'humor', 'or', None, 'not', 'is', 'etc']\n"
          ]
        }
      ]
    }
  ]
}